---
title: "TO 628 Group 5 Project: Faulty Steel Plates Prediction"
author: "Add names here, Shraddha Ramesh, Meng-Ni Ho, Jen Hung"
date: 'Analyses completed: `r format(Sys.Date(), "%Y-%m-%d")`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project Description

Dataset resource: [from kaggle](https://www.kaggle.com/uciml/faulty-steel-plates?fbclid=IwAR1_GKUHnj6D0haU8UuIj24jjeXzXtkwghQAI-y9y_FcXLrOnOIg3W1Kwd8)



## Load data + Library
```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(naniar)
library(C50)
library(e1071) 
library(caret)
library(class)
library(gmodels)
faults <- read.csv("faults_recode.csv")

```

## Data Cleaning
```{r}

faults %>% glimpse()
faults %>% miss_var_summary() #No NAs
#any(is.na(faults))

#Type of steel is merged into a single column and mase to factor
faults$SteelType<-ifelse(faults$TypeOfSteel_A300==1,1,0)
#faults$SteelType<-as.factor(faults$SteelType)

# select relevant variables
# outcome: sdata$label
new.faults = faults %>% select(-c(Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, Other_Faults, 
                            TypeOfSteel_A300, TypeOfSteel_A400))
new.faults$label = as.factor(new.faults$label)
```

## Split training and testing set: ratio 7:3
training set: N = 1358
testing set: N = 583
```{r}
# split train and test dataset
set.seed(40)
train_ind <- sample(seq_len(nrow(new.faults)), size = as.integer(dim(new.faults)[1]*0.7))

train <- new.faults[train_ind,]
test <- new.faults[-train_ind,]
```


## Logistic Regression

```{r}
#Logit model for Pastry Faults

faults_logit <- faults[c(1:11,14:26,36,35,28)]
faults_logit$label<- NULL
#faults_logit$Pastry<- as.factor(ifelse(faults_logit$Pastry==1,'Pastry','Not Pastry'))
faults_logit %>% glimpse()
fault_logit_model <- glm(Pastry~X_Minimum + X_Maximum + Y_Minimum + Y_Maximum +Pixels_Areas + X_Perimeter + Y_Perimeter + Sum_of_Luminosity + Minimum_of_Luminosity + Maximum_of_Luminosity + Length_of_Conveyer + Steel_Plate_Thickness + Edges_Index+ Square_Index + Edges_Y_Index + LogOfAreas, data = faults_logit, family = 'binomial')
summary(fault_logit_model)


```


## Support Vector Machine: Linear Kernal
```{r}
# start training using linear kernal
system.time({
  svm.linear <- svm(formula = label ~ ., 
                 data = train, 
                 type = 'C-classification', 
                 kernel = 'linear') 
})

# predict test result
svm.linear.pred <- predict(svm.linear, newdata = test %>% select(-label)) 

# confusion matrix
cm.svm.linear <- confusionMatrix(svm.linear.pred, as.factor(test[, "label"]))
accuracy.svm.linear <- round(as.numeric(cm.svm.linear$overall[1])*100, digits = 2)
pred.table.svm.linear <- cm.svm.linear$table
print(pred.table.svm.linear)
```


## Support Vector Machine: Radial Kernal
```{r}
system.time({
  svm.radial <- svm(formula = label ~ ., 
                 data = train, 
                 type = 'C-classification', 
                 kernel = 'radial') 
})

# predict test result
svm.radial.pred <- predict(svm.radial, newdata = test %>% select(-label)) 

# confusion matrix
cm.svm.radial <- confusionMatrix(svm.radial.pred, as.factor(test[, "label"]))
accuracy.svm.radial <- round(as.numeric(cm.svm.radial$overall[1])*100, digits = 2)
pred.table.svm.radial <- cm.svm.radial$table
print(pred.table.svm.radial)
```

## k-Nearest-Neighbors (KNN) 
```{r}
# seperate the variables and labels for train and test dataset

train_label <- train[,26]
train_KNN <- train[,-26]
test_label <- test[,26]
test_KNN <- test[,-26]


#do the z-Score Standardization for dataset
standardized.train_KNN <- scale(train_KNN[,])
standardized.test_KNN <- scale(test_KNN[,])

KNN_pred <- knn(train = standardized.train_KNN, test = standardized.test_KNN, cl = train_label, k=8 )

# Accuracy
mean(test_label== KNN_pred)

# find the best k-value
accuracy.rate = NULL
for(i in 1:20){
    KNN_pred_find = knn(train = standardized.train_KNN, test = standardized.test_KNN, cl = train_label, k=i)
    accuracy.rate[i] = 1-(mean(test_label != KNN_pred_find))
}
print(accuracy.rate)


```



